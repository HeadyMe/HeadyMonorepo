# --- SECURITY TEMPLATE ---
# 1. Copy this file to '.env'
# 2. Replace placeholders with actual keys
# 3. NEVER commit the .env file to Git

# Hugging Face (Required for Model Inference)
# Get from: https://huggingface.co/settings/tokens
HF_TOKEN=hf_your_token_here

# Google Gemini (Required for Reasoning)
# Get from: https://aistudio.google.com/app/apikey
GOOGLE_API_KEY=AIzaSy_your_google_key_here

# Heady Services (Required for MCP Integration)
# Internal Team Key
HEADY_API_KEY=heady_live_your_key_here

# Optional: GitHub Enterprise Token
GH_TOKEN=

# --- Server Configuration ---
PORT=3300
NODE_ENV=development

# --- Model Configuration ---
HF_TEXT_MODEL=gpt2
HF_EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2

# --- Rate Limiting ---
HEADY_RATE_LIMIT_WINDOW_MS=60000
HEADY_RATE_LIMIT_MAX=120

# --- Concurrency Limits ---
HF_MAX_CONCURRENCY=4
HEADY_PY_MAX_CONCURRENCY=2

# --- Python Worker Configuration ---
HEADY_PYTHON_BIN=python
HEADY_PY_WORKER_TIMEOUT_MS=90000

# --- QA Configuration ---
HEADY_QA_BACKEND=auto
HEADY_QA_MAX_NEW_TOKENS=256
HEADY_QA_MAX_QUESTION_CHARS=4000
HEADY_QA_MAX_CONTEXT_CHARS=12000

# --- Admin UI Configuration ---
HEADY_ADMIN_ROOT=
HEADY_ADMIN_ALLOWED_PATHS=
HEADY_ADMIN_MAX_BYTES=512000
HEADY_ADMIN_OP_LOG_LIMIT=2000
HEADY_ADMIN_OP_LIMIT=50

# --- GPU Configuration ---
HEADY_ADMIN_ENABLE_GPU=false
REMOTE_GPU_HOST=
REMOTE_GPU_PORT=
GPU_MEMORY_LIMIT=
ENABLE_GPUDIRECT=false

# --- CORS Configuration ---
HEADY_CORS_ORIGINS=http://localhost:3000,http://localhost:3300

# --- Proxy Configuration ---
HEADY_TRUST_PROXY=false

# --- MCP Configuration ---
HEADY_MCP_SERVER_TIMEOUT_MS=30000
HEADY_MCP_MAX_CONCURRENT_REQUESTS=10

# --- Logging Configuration ---
HEADY_LOG_LEVEL=info
HEADY_LOG_FORMAT=json

# --- Security Configuration ---
HEADY_SESSION_SECRET=your_session_secret_here
HEADY_JWT_EXPIRES_IN=24h
